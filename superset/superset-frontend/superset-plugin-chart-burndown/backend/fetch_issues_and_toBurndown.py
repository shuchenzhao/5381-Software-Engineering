#!/usr/bin/env python3
import requests
import pandas as pd
from typing import List, Dict, Any
import os
from datetime import datetime, timedelta, timezone
from dotenv import load_dotenv
import json
import argparse
from pathlib import Path
import re

# ===================================================
# Step 0: ç¯å¢ƒå˜é‡åŠ è½½
# ===================================================
# ä½¿ç”¨ python-dotenv åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
# è¿™é‡Œä¸»è¦æ˜¯ GitHub è®¿é—® Token
load_dotenv()
GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN")
if not GITHUB_TOKEN:
    raise ValueError("âŒ GITHUB_TOKEN not found in environment variables (.env)")

# --- æ•°æ®ä¿å­˜ç›®å½• ---
DATA_DIR = "data"
os.makedirs(DATA_DIR, exist_ok=True)  # å¦‚æœä¸å­˜åœ¨åˆ™åˆ›å»º

# ===================================================
# Step 1: æ‹‰å– GitHub Issues
# ===================================================
def fetch_all_issues(owner: str, repo: str, token: str, days: int = 90) -> List[Dict[str, Any]]:
    """
    æ‹‰å–æŒ‡å®š GitHub ä»“åº“çš„æ‰€æœ‰ Issueï¼ˆä¸åŒ…å« Pull Requestï¼‰
    åªæ‹‰å–æœ€è¿‘ `days` å¤©å†…åˆ›å»ºçš„ Issues
    """
    github_api_url = f"https://api.github.com/repos/{owner}/{repo}/issues"
    all_issues = []  # ä¿å­˜æ‰€æœ‰æ‹‰å–åˆ°çš„ issue
    page = 1  # åˆ†é¡µç´¢å¼•

    headers = {
        "Authorization": f"token {token}",
        "Accept": "application/vnd.github.v3+json"
    }

    # since å‚æ•°è¡¨ç¤ºä»å¤šä¹…ä»¥å‰å¼€å§‹æ‹‰å–
    since = (datetime.now(timezone.utc) - timedelta(days=days)).isoformat()
    print(f"ğŸ“¡ Fetching Issues from {owner}/{repo}")
    print(f"   â†’ Since: {since}")

    while True:
        # æ„é€ è¯·æ±‚å‚æ•°
        params = {
            "state": "all",         # æ‹‰å–æ‰€æœ‰çŠ¶æ€çš„ issue
            "per_page": 100,        # æ¯é¡µæœ€å¤š 100 æ¡
            "page": page,           # å½“å‰é¡µç 
            "sort": "created",      # æŒ‰åˆ›å»ºæ—¶é—´æ’åº
            "direction": "desc",    # æœ€æ–°åˆ›å»ºçš„ issue åœ¨å‰
            "since": since          # ä»æŒ‡å®šæ—¥æœŸå¼€å§‹æ‹‰å–
        }

        response = requests.get(github_api_url, headers=headers, params=params)
        if response.status_code != 200:
            try:
                error_detail = response.json()
            except requests.exceptions.JSONDecodeError:
                error_detail = response.text or "No detailed response body."
            print(f"âŒ GitHub API è¯·æ±‚å¤±è´¥ã€‚çŠ¶æ€ç : {response.status_code}. å“åº”: {error_detail}")
            return []

        current_issues = response.json()
        if not current_issues:  # å¦‚æœå½“å‰é¡µæ²¡æœ‰æ•°æ®ï¼Œç»“æŸå¾ªç¯
            break

        # ç­›é€‰æ‰æ‹‰å–è¯·æ±‚ PRï¼Œåªä¿ç•™ issue
        issues_only = [issue for issue in current_issues if "pull_request" not in issue]
        all_issues.extend(issues_only)

        # æ£€æŸ¥æ˜¯å¦æœ‰ä¸‹ä¸€é¡µ
        if 'link' in response.headers and 'rel="next"' in response.headers['link']:
            page += 1
        else:
            break

    return all_issues

# ===================================================
# Step 1b: æ•°æ®æ ¼å¼åŒ–å‡½æ•°
# ===================================================
def json_to_list_of_dicts(issues_data: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    å°†åŸå§‹ GitHub Issues JSON æ•°æ®è½¬æ¢ä¸ºæ‰å¹³åŒ–å­—å…¸åˆ—è¡¨
    åªä¿ç•™æŒ‡å®šæ ¸å¿ƒå­—æ®µ
    """
    if not issues_data:
        return []

    CORE_COLUMNS = {
        'number': 'issue_number',
        'state': 'issue_state',
        'created_at': 'created_at',
        'closed_at': 'closed_at',
        'user_login': 'creator_login',
        'milestone_title': 'milestone_title',
        'title': 'title'
    }

    try:
        df = pd.json_normalize(issues_data, sep='_')  # æ‰å¹³åŒ– JSON
        cols_to_select = {k: v for k, v in CORE_COLUMNS.items() if k in df.columns}
        df = df.rename(columns=cols_to_select)[list(cols_to_select.values())]

        # å°† NaN æ›¿æ¢ä¸º Noneï¼Œé¿å… Pandas é»˜è®¤å¡«å……
        for col in [c for c in df.columns if 'at' in c]:
            df[col] = df[col].where(pd.notnull(df[col]), None)

        return df.to_dict('records')
    except Exception as e:
        print(f"âš ï¸ Error during data normalization: {e}")
        return []

# ===================================================
# Step 1c: ä¿å­˜åˆ°æœ¬åœ° JSON
# ===================================================
def save_issues_to_local(data: List[Dict[str, Any]], owner: str, repo: str) -> str:
    """
    ä¿å­˜ Issues æ•°æ®åˆ°æœ¬åœ° JSON æ–‡ä»¶
    æ–‡ä»¶ååŒ…å« ownerã€repo å’Œæ—¶é—´æˆ³
    """
    timestamp = datetime.now().strftime("%Y-%m-%d_%H%M%S")
    filename = f"github_issues_{owner}_{repo}_{timestamp}.json"
    filepath = os.path.join(DATA_DIR, filename)

    with open(filepath, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"ğŸ’¾ Issues data saved to {filepath}")
    return filepath

# ===================================================
# Step 2: è½¬æ¢ä¸ºç‡ƒå°½å›¾æ•°æ®
# ===================================================
def load_issues(json_file):
    """è¯»å–æœ¬åœ° JSON æ–‡ä»¶"""
    with open(json_file, "r", encoding="utf-8") as f:
        return json.load(f)

def compute_remaining(issues, project_start_date):
    """
    è®¡ç®—æ¯å¤©å‰©ä½™ Issue æ•°ï¼Œç”¨äºç»˜åˆ¶ç‡ƒå°½å›¾
    è¿”å›åˆ—è¡¨ï¼š
    [{"ds": "2025-10-01", "remaining": 100}, ...]
    """
    # 1ï¸âƒ£ å°†å­—ç¬¦ä¸²æ—¥æœŸè½¬æ¢ä¸º datetime å¯¹è±¡
    for issue in issues:
        issue["created_at_dt"] = datetime.strptime(issue["created_at"][:10], "%Y-%m-%d")
        issue["closed_at_dt"] = None
        if issue.get("closed_at"):
            issue["closed_at_dt"] = datetime.strptime(issue["closed_at"][:10], "%Y-%m-%d")

    # 2ï¸âƒ£ æ—¶é—´èŒƒå›´ï¼šä»æœ€æ—©åˆ›å»ºçš„ issue åˆ°ä»Šå¤©
    min_date = min(issue["created_at_dt"] for issue in issues)
    max_date = datetime.today()

    # 3ï¸âƒ£ æ„å»ºæ¯æ—¥å‰©ä½™ä»»åŠ¡åˆ—è¡¨
    remaining_data = []
    current_date = min_date
    while current_date <= max_date:
        # å·²åˆ›å»ºçš„ issue æ•°
        created_count = sum(1 for issue in issues if issue["created_at_dt"] <= current_date)
        # å·²å…³é—­çš„ issue æ•°
        closed_count = sum(1 for issue in issues if issue["closed_at_dt"] and issue["closed_at_dt"] <= current_date)
        # å‰©ä½™æœªå®Œæˆçš„ issue
        remaining_data.append({
            "ds": current_date.strftime("%Y-%m-%d"),
            "remaining": created_count - closed_count
        })
        current_date += timedelta(days=1)

    # 4ï¸âƒ£ æˆªå–é¡¹ç›®å¼€å§‹æ—¥æœŸåçš„æ•°æ®
    start_dt = datetime.strptime(project_start_date, "%Y-%m-%d")
    remaining_data = [d for d in remaining_data if d["ds"] >= start_dt.strftime("%Y-%m-%d")]

    return remaining_data

def save_ts(remaining_data, output_file):
    """
    ä¿å­˜ç‡ƒå°½å›¾æ•°æ®åˆ°ç‹¬ç«‹ TS æ–‡ä»¶
    æ ¼å¼ï¼š
    export const burndownData = [{ ds: '2025-10-01', remaining: 100 }, ...]
    """
    ts_content = "// ç”Ÿæˆäº {}\n".format(datetime.now().strftime("%Y-%m-%d %H:%M:%S"))
    ts_content += "export const burndownData = [\n"
    for d in remaining_data:
        ts_content += f"  {{ ds: '{d['ds']}', remaining: {d['remaining']} }},\n"
    ts_content += "];\n"

    with open(output_file, "w", encoding="utf-8") as f:
        f.write(ts_content)

# ===================================================
# Step 3: æ›´æ–° Burndown.stories.tsx ç¤ºä¾‹
# ===================================================
def update_burndown_stories(stories_file: str, remaining_data: list):
    """
    æ›¿æ¢ Burndown.stories.tsx ä¸­çš„ const data = [...] å†…å®¹
    """
    path = Path(stories_file)
    content = path.read_text(encoding="utf-8")

    ts_data_str = "\n".join(
        f"  {{ ds: '{d['ds']}', remaining: {d['remaining']} }}," for d in remaining_data
    )

    # ä½¿ç”¨æ­£åˆ™åŒ¹é…å¹¶æ›¿æ¢åŸæœ‰ data å—
    pattern = re.compile(r"(const\s+data\s*=\s*\[)(.*?)(\];)", re.DOTALL)
    new_content, count = pattern.subn(r"\1\n" + ts_data_str + r"\n\3", content)

    if count == 0:
        raise ValueError("æœªæ‰¾åˆ° const data = [...] ä»£ç å—ï¼Œè¯·æ£€æŸ¥ Burndown.stories.tsx æ–‡ä»¶æ ¼å¼ã€‚")

    path.write_text(new_content, encoding="utf-8")
    print(f"Burndown.stories.tsx å·²æ›´æ–°ï¼Œæ›¿æ¢ {count} ä¸ª data å—")

# ===================================================
# ä¸»å‡½æ•°
# ===================================================
def main():
    parser = argparse.ArgumentParser(description="Fetch GitHub issues and generate burndown data")
    parser.add_argument("--owner", required=True, help="GitHub repository owner")
    parser.add_argument("--repo", required=True, help="GitHub repository name")
    parser.add_argument("--days", type=int, default=90, help="Days of issue history to fetch")
    parser.add_argument("--start", required=True, help="Project start date, e.g. 2025-10-01")
    parser.add_argument("--output", required=True, help="Path to output TS file")
    args = parser.parse_args()

    # Step 1: Fetch issues
    issues_json_data = fetch_all_issues(
        owner=args.owner,
        repo=args.repo,
        token=GITHUB_TOKEN,
        days=args.days
    )
    if not issues_json_data:
        print("âš ï¸ No issues fetched or API request failed.")
        return

    # æ ¼å¼åŒ–å­—æ®µ
    final_data_list = json_to_list_of_dicts(issues_json_data)
    # ä¿å­˜æœ€æ–° JSON æ–‡ä»¶
    latest_json_file = save_issues_to_local(final_data_list, owner=args.owner, repo=args.repo)
    print(f"âœ… Latest JSON file: {latest_json_file}")
    print(f"âœ… Total issues processed: {len(final_data_list)}")

    # Step 2: Generate burndown
    issues = load_issues(latest_json_file)
    remaining_data = compute_remaining(issues, args.start)
    save_ts(remaining_data, args.output)
    print(f"ğŸ’¾ Burndown data saved to {args.output}")

    # Step 3: Update stories
    stories_file = "../src/stories/Burndown.stories.tsx"
    update_burndown_stories(stories_file, remaining_data)

if __name__ == "__main__":
    main()
